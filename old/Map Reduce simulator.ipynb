{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating MapReduce jobs + Spark TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a folder, place the file 5000-8.txt and the notebook in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "with open('5000-8.txt', encoding = \"ISO-8859-1\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCount\n",
    "\n",
    "##### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "#Compute a list \n",
    "WC_mapper_out = []\n",
    "# input comes from STDIN (standard input)\n",
    "for line in lines:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    line = re.sub('['+string.punctuation+']', '', line)\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        WC_mapper_out.append('%s\\t%s\\n' % (word, 1))\n",
    "print(WC_mapper_out[1:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replicate the sorting of hadoop\n",
    "WC_mapper_out.sort()\n",
    "\n",
    "#Reducer code\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "WC_reducer_out =[]\n",
    "# input comes from STDIN\n",
    "for line in mapper_out:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            reducer_out.append('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    WC_reducer_out.append('%s\\t%s' % (current_word, current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for lines in reducer_out[31000:32000]:\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling\n",
    "\n",
    "#### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import string\n",
    "import re\n",
    "\n",
    "#Compute a list \n",
    "RS_mapper_out = []\n",
    "# input comes from STDIN (standard input)\n",
    "for line in lines:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    line = re.sub('['+string.punctuation+']', '', line)\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "    #add a random number between 0 and 10000 as a key\n",
    "        RS_mapper_out.append('%s\\t%s\\n' % (word, randint(0,10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(i) for i in RS_mapper_out[1:1000]]\n",
    "\n",
    "print(RS_mapper_out[1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "## replicate the sorting of hadoop\n",
    "RS_mapper_out.sort()\n",
    "\n",
    "RS_reducer_out =[]\n",
    "\n",
    "# input comes from STDIN\n",
    "key_val = random.sample(range(0, 10000), 10)\n",
    "out_key = random.randint(0,10000)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for line in RS_mapper_out:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    couple = line.split('\\t')\n",
    "    word = couple[0]\n",
    "    key = int(couple[1])\n",
    "\n",
    "    # print(key_val,key)\n",
    "    if key in key_val:\n",
    "        RS_reducer_out.append('%s\\t%s' % (word, out_key))\n",
    "        \n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_reducer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "## replicate the sorting of hadoop\n",
    "RS_mapper_out.sort()\n",
    "\n",
    "RS_reducer_out =[]\n",
    "\n",
    "# input comes from STDIN\n",
    "key_val = set(random.sample(range(0, 10000), 10))\n",
    "out_key = random.randint(0,10000)\n",
    "start_time = time.time()\n",
    "\n",
    "for line in RS_mapper_out:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    couple = line.split('\\t')\n",
    "    word = couple[0]\n",
    "    key = int(couple[1])\n",
    "\n",
    "    # print(key_val,key)\n",
    "    try: \n",
    "        if key in key_val:\n",
    "            RS_reducer_out.append('%s\\t%s' % (word, out_key))\n",
    "    except ValueError: \n",
    "        continue\n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_reducer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second option with set is much more efficient (confirming https://stackoverflow.com/questions/7571635/fastest-way-to-check-if-a-value-exist-in-a-list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYSPARK TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw = sc.textFile(\"5000-8.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw.take(5)\n",
    "\n",
    "#Cleaning text and removing punctuation:\n",
    "\n",
    "text_clean = text_raw.map(lambda x: ''.join([ c for c in x if (c.isalnum() or c==' ')]))\n",
    "text_clean = text_clean.filter(lambda x: x != '').map(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text_clean.flatMap(lambda x: x.split())\n",
    "doc_nb = 5\n",
    "sample_size = 0.001\n",
    "\n",
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(words.sample(False,sample_size)) for i in range(doc_nb)]\n",
    "documents = [sc.parallelize(documents[i].map(lambda x: (i,x)).collect()) for i in range(doc_nb)]\n",
    "documents = sc.union(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doclist = documents.groupByKey().map(lambda x: list(x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built in hashing TFIDF on rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(doclist)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tfidf:\")\n",
    "for each in tfidf.collect():\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF IDF built in on Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "test = documents.groupByKey().mapValues(list)\n",
    "test = test.toDF([\"id\",\"words\"])\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(test)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"text\", outputCol=\"features\")\n",
    "model = vectorizer.fit(test)\n",
    "result = model.transform(test)\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"outfeatures\")\n",
    "idfModel = idf.fit(result)\n",
    "rescaledData = idfModel.transform(result)\n",
    "rescaledData.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "doc_word_count = documents.map(lambda x: ((x[0],x[1]),1)).reduceByKey(lambda x,y: x+y)\n",
    "occ_count = doc_word_count.map(lambda x: (x[0][1],1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "IDF = occ_count.map(lambda x : (x[0],math.log(doc_nb/x[1])))\n",
    "IDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_word_count.sortBy(lambda x: x[1],False).collect()\n",
    "joined = doc_word_count.map(lambda x: (x[0][1],(x[0][0],x[1]))).join(IDF)\n",
    "TFIDF_scores = joined.map(lambda x: (x[0],(x[1][0][0],x[1][0][1]*x[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_scores.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using dataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_df = doc_word_count.map(lambda x:(x[0][0],x[0][1],x[1])).toDF([\"doc_id\",\"word\",\"occurence\"])\n",
    "idf_df = IDF.toDF([\"word\",\"IDF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = wc_df.join(idf_df,on=\"word\")\n",
    "out = out.withColumn(\"TFIDF\",out.IDF*out.occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first\n",
    "\n",
    "out_pivot = out.groupBy(\"doc_id\").pivot(\"word\").agg(first(\"TFIDF\"))\n",
    "print((out_pivot.count(), len(out_pivot.columns)))\n",
    "out_pivot.select(\"then\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.where(out.word==\"then\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
